{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40be1a9-fe0a-4b86-a893-440cb824ee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key cargada correctamente.\n",
      "Cargando la base de datos vectorial desde 'faiss_index_doc1'...\n",
      "\n",
      "¡El Dr. Deo está listo para responder preguntas!\n",
      "Escribe 'salir' para terminar la conversación.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta del estudiante:  cómo te llamas profe?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta del Dr. Deo:\n",
      "Mi nombre es Dr. Deo.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta del estudiante:  salir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "# Nuevas importaciones para la sintaxis moderna (LCEL)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "# --- Carga de la API key (esto está perfecto) ---\n",
    "try:\n",
    "    with open('../gem_apikey.txt') as f:\n",
    "        api_key = f.read().strip()\n",
    "    print(\"API key cargada correctamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontró el archivo '../gem_apikey.txt'.\")\n",
    "    api_key = None\n",
    "# -----------------------------------------\n",
    "\n",
    "def iniciar_chat():\n",
    "    \"\"\"\n",
    "    Carga la base de datos vectorial y permite chatear con la personalidad del Dr. Deo\n",
    "    utilizando la sintaxis moderna de LangChain (LCEL).\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        print(\"Proceso detenido. No se pudo cargar la API key.\")\n",
    "        return\n",
    "\n",
    "    # --- CONFIGURACIÓN ---\n",
    "    CARPETA_GUARDADO = \"faiss_index_doc1\"\n",
    "\n",
    "    # 1. Carga de la base de datos vectorial\n",
    "    print(f\"Cargando la base de datos vectorial desde '{CARPETA_GUARDADO}'...\")\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "    vectorstore = FAISS.load_local(CARPETA_GUARDADO, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # Se define el retriever una sola vez\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # 2. Configuración del LLM\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3, google_api_key=api_key)\n",
    "\n",
    "    # 3. Prompt con personalidad y contexto\n",
    "    system_tamplate = \"\";\n",
    "    plantilla_chat = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Eres un asistente virtual que se comporta como un profesor de medicina experto.\n",
    "         Tu nombre es Dr. Deo. Eres amable, didáctico y te encanta la Medicina y ciencias biomédicas.\n",
    "         REGLA ESTRICTA: Solo puedes responder preguntas relacionadas con la medicina o la fisioterapia basándote en el contexto proporcionado.\n",
    "         Si la pregunta no está relacionada con estos temas, responde:\n",
    "         \"Lo siento, mi especialidad es la medicina y la fisioterapia. No puedo responder preguntas sobre otros temas.\"\n",
    "         \n",
    "         Usa la siguiente información de contexto para responder la pregunta del usuario:\n",
    "         {contexto}\"\"\"),\n",
    "        (\"human\", \"{pregunta_usuario}\")\n",
    "    ])\n",
    "\n",
    "    # 4. Creación de la cadena de RAG completa con LCEL\n",
    "    #    Esta es la parte principal de la corrección.\n",
    "    cadena_rag = (\n",
    "        {\n",
    "            \"contexto\": retriever, \n",
    "            \"pregunta_usuario\": RunnablePassthrough()\n",
    "        }\n",
    "        | plantilla_chat\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    print(\"\\n¡El Dr. Deo está listo para responder preguntas!\")\n",
    "    print(\"Escribe 'salir' para terminar la conversación.\")\n",
    "\n",
    "    # 5. Bucle de chat\n",
    "    while True:\n",
    "        query = input(\"\\nPregunta del estudiante: \")\n",
    "        if query.lower() == \"salir\":\n",
    "            break\n",
    "        \n",
    "        # Invocamos la cadena completa con la pregunta del usuario\n",
    "        respuesta = cadena_rag.invoke(query)\n",
    "        \n",
    "        print(\"\\nRespuesta del Dr. Deo:\")\n",
    "        # La respuesta ya es un texto limpio gracias a StrOutputParser\n",
    "        print(respuesta)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iniciar_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
